Dagster ETL Pipeline Implementation

This directory contains the Dagster implementation of the ETL pipeline, focusing on asset-based design, explicit data dependencies, and strong observability. The pipeline reuses the same shared ETL logic as the Airflow and Prefect implementations, ensuring consistent data processing and output parity.

# Directory Structure

The Dagster implementation emphasizes clarity and modularity.

job.py defines the Dagster job and ops

shared/etl_logic.py contains the reusable transformation logic

requirements.txt specifies Python dependencies

README.md documents execution and verification steps

# Execution Environment

This Dagster pipeline was executed locally using Dagster’s native development server (dagster dev).
Docker was not used for Dagster in this project, as the local execution environment was sufficient.

# Setup Instructions

Navigate to the Dagster directory.

cd dagster


Install the required Python dependencies.

pip install -r requirements.txt

Running the Pipeline

Start the Dagster development server.

dagster dev


Open the Dagit UI in a browser:

http://localhost:3000


From the Dagit UI:

Select the defined job

Launch a run with the configured parameters

Monitor execution and logs in real time

UI-Based Execution and Monitoring

Dagster provides Dagit, a web-based UI focused on observability and data lineage.

Using Dagit, the following were verified:

Job execution order and dependencies

Op-level execution status and logs

Retry behavior for failed ops

Successful materialization of outputs

# Retry Logic Implementation

Retry behavior is implemented at the op level using Dagster’s retry policy. This ensures that only the failing operation is retried rather than re-executing the entire pipeline.

# Backfill Execution

Backfill execution is supported by running the same job multiple times with different configurations. Since the pipeline is deterministic and parameter-driven, historical runs can be reproduced reliably using the Dagit UI.

# Output Validation

The Parquet output generated by Dagster matches exactly with the outputs produced by the Airflow and Prefect pipelines when executed with identical input data and parameters.

# Design Notes

This implementation highlights Dagster’s strengths in data-centric orchestration, explicit dependency modeling, and strong UI-driven observability while maintaining a clean separation between computation and orchestration logic.